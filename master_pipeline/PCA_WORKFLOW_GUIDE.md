# PCA Pipeline Workflow - Visual Guide

## ğŸ¯ Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PCA ANOMALY DETECTION PIPELINE               â”‚
â”‚                                                                 â”‚
â”‚  Input: 870 TSB-AD-U Datasets â†’ Process â†’ Analyze â†’ Compare    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š Complete Workflow

```
START
  â”‚
  â”œâ”€â–º STEP 1: Data Processing
  â”‚    â”œâ”€ Load 870 CSV files from Datasets/TSB-AD-U/
  â”‚    â”œâ”€ Extract features and labels
  â”‚    â”œâ”€ Calculate metadata (length, anomaly ratio, etc.)
  â”‚    â””â”€ Compute optimal sliding window per dataset
  â”‚
  â”œâ”€â–º STEP 2: PCA Execution
  â”‚    â”œâ”€ For each dataset:
  â”‚    â”‚   â”œâ”€ Run PCA (unsupervised)
  â”‚    â”‚   â”œâ”€ Generate anomaly scores
  â”‚    â”‚   â”œâ”€ Save scores (.npy)
  â”‚    â”‚   â””â”€ Measure runtime
  â”‚    â””â”€ Checkpoint every 10 datasets
  â”‚
  â”œâ”€â–º STEP 3: Evaluation
  â”‚    â”œâ”€ For each dataset:
  â”‚    â”‚   â”œâ”€ Compute VUS-PR (main metric)
  â”‚    â”‚   â”œâ”€ Compute VUS-ROC
  â”‚    â”‚   â”œâ”€ Compute Affiliation metrics
  â”‚    â”‚   â””â”€ Save to results CSV
  â”‚    â””â”€ Generate summary statistics
  â”‚
  â”œâ”€â–º STEP 4: Statistical Analysis
  â”‚    â”œâ”€ Overall performance stats
  â”‚    â”œâ”€ Distribution analysis
  â”‚    â”œâ”€ Category breakdown
  â”‚    â”œâ”€ Correlation analysis
  â”‚    â”œâ”€ Top/bottom performers
  â”‚    â””â”€ Generate 8+ visualizations
  â”‚
  â””â”€â–º STEP 5: Benchmark Comparison
       â”œâ”€ Load TSB-AD benchmark results
       â”œâ”€ Rank PCA vs 30+ methods
       â”œâ”€ Statistical significance tests
       â””â”€ Generate comparison plots
       
END (All results saved)
```

## ğŸ—‚ï¸ Directory Structure After Completion

```
master_pipeline/
â”‚
â”œâ”€ Run_PCA_Pipeline.py              â† Main execution script
â”œâ”€ PCA_Statistical_Analysis.ipynb  â† Analysis notebook
â”œâ”€ Compare_PCA_with_Benchmark.py   â† Comparison tool
â”œâ”€ PCA_Quick_Start.py               â† One-command launcher
â”œâ”€ PCA_Pipeline_README.md           â† Detailed documentation
â”œâ”€ PCA_PIPELINE_SUMMARY.md          â† This summary
â”‚
â””â”€ eval/
   â””â”€ PCA_pipeline/
      â”‚
      â”œâ”€ PCA_pipeline.log                  â† Execution log
      â”‚
      â”œâ”€ scores/                            â† 870 anomaly score files
      â”‚  â”œâ”€ 001_NAB_id_1_*.npy
      â”‚  â”œâ”€ 002_NAB_id_2_*.npy
      â”‚  â””â”€ ...
      â”‚
      â””â”€ metrics/
         â”‚
         â”œâ”€ PCA_all_results.csv            â† â­ Main results file
         â”œâ”€ PCA_summary_report.csv         â† Summary statistics
         â”œâ”€ PCA_failed_files.csv           â† Error tracking (if any)
         â”‚
         â”œâ”€ distribution_analysis.png       â† Distribution plots
         â”œâ”€ boxplot_analysis.png           â† Box plots
         â”œâ”€ performance_by_category.png    â† Category analysis
         â”œâ”€ correlation_matrix.png         â† Correlation heatmap
         â”œâ”€ scatter_relationships.png      â† Scatter plots
         â”œâ”€ top_bottom_performers.png      â† Best/worst datasets
         â”œâ”€ qq_plots.png                   â† Normality tests
         â”œâ”€ comprehensive_dashboard.png    â† Full dashboard
         â”‚
         â””â”€ comparisons/                    â† Benchmark comparisons
            â”œâ”€ pca_vs_top_methods.png
            â”œâ”€ method_ranking.png
            â”œâ”€ pca_vs_best.png
            â””â”€ combined_results.csv
```

## ğŸ”„ Execution Flow

### Option 1: Manual (Step by Step)

```bash
# Terminal
cd master_pipeline

# Step 1: Run pipeline
python Run_PCA_Pipeline.py
# â±ï¸ ~20-30 minutes
# âœ“ Generates scores + metrics

# Step 2: Analyze
jupyter notebook PCA_Statistical_Analysis.ipynb
# â±ï¸ ~5 minutes
# âœ“ Generates all plots

# Step 3: Compare
python Compare_PCA_with_Benchmark.py
# â±ï¸ ~1 minute
# âœ“ Generates comparison plots
```

### Option 2: Quick Start (Automated)

```bash
# Terminal
cd master_pipeline
python PCA_Quick_Start.py

# Prompts you through:
# 1. Run pipeline
# 2. Launch analysis notebook
# All automatic!
```

## ğŸ“ˆ Data Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Dataset    â”‚  001_NAB_id_1_Facility_tr_1007_1st_2014.csv
â”‚   (CSV)      â”‚  [time_series, labels]
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â–º Load & Parse
       â”‚   â”œâ”€ data: (2014, 1) float array
       â”‚   â””â”€ label: (2014,) int array
       â”‚
       â”œâ”€â–º Metadata Extraction
       â”‚   â”œâ”€ data_length: 2014
       â”‚   â”œâ”€ num_features: 1
       â”‚   â”œâ”€ num_anomalies: 7
       â”‚   â”œâ”€ anomaly_ratio: 0.0035
       â”‚   â””â”€ sliding_window: 67 (auto-computed)
       â”‚
       â”œâ”€â–º PCA Execution
       â”‚   â”œâ”€ Fit PCA on data
       â”‚   â”œâ”€ Transform to components
       â”‚   â”œâ”€ Compute reconstruction error
       â”‚   â””â”€ anomaly_scores: (2014,) float array
       â”‚
       â”œâ”€â–º Save Scores
       â”‚   â””â”€ scores/001_NAB_id_1_*.npy
       â”‚
       â”œâ”€â–º Evaluation
       â”‚   â”œâ”€ Compute VUS-PR: 0.xxxx
       â”‚   â”œâ”€ Compute VUS-ROC: 0.xxxx
       â”‚   â””â”€ Compute other metrics
       â”‚
       â””â”€â–º Append to Results
           â””â”€ PCA_all_results.csv (new row)

[Repeat for all 870 datasets]
```

## ğŸ¨ Analysis Outputs Visual Map

```
PCA_Statistical_Analysis.ipynb
â”‚
â”œâ”€ Section 1: Load Results
â”‚  â””â”€ Display: First 5 rows, column info
â”‚
â”œâ”€ Section 2: Overall Statistics
â”‚  â””â”€ Print: Mean, std, median, min, max for all metrics
â”‚
â”œâ”€ Section 3: Distribution Analysis
â”‚  â””â”€ Plot: distribution_analysis.png
â”‚     â”œâ”€ VUS-PR histogram + KDE
â”‚     â”œâ”€ VUS-ROC histogram + KDE
â”‚     â”œâ”€ Affiliation Precision histogram + KDE
â”‚     â””â”€ Affiliation Recall histogram + KDE
â”‚
â”œâ”€ Section 4: Box Plot Analysis
â”‚  â””â”€ Plot: boxplot_analysis.png
â”‚     â””â”€ All metrics in one figure
â”‚
â”œâ”€ Section 5: Category Performance
â”‚  â””â”€ Plot: performance_by_category.png
â”‚     â”œâ”€ Box plot by category
â”‚     â””â”€ Bar chart with error bars
â”‚
â”œâ”€ Section 6: Correlation Analysis
â”‚  â”œâ”€ Plot: correlation_matrix.png (heatmap)
â”‚  â””â”€ Plot: scatter_relationships.png
â”‚     â”œâ”€ VUS-PR vs Data Length
â”‚     â”œâ”€ VUS-PR vs Anomaly Ratio
â”‚     â”œâ”€ Runtime vs Data Length
â”‚     â””â”€ VUS-PR vs VUS-ROC
â”‚
â”œâ”€ Section 7: Top/Bottom Performers
â”‚  â”œâ”€ Print: Top 10 datasets table
â”‚  â”œâ”€ Print: Bottom 10 datasets table
â”‚  â””â”€ Plot: top_bottom_performers.png
â”‚
â”œâ”€ Section 8: Statistical Tests
â”‚  â”œâ”€ Print: Shapiro-Wilk results
â”‚  â””â”€ Plot: qq_plots.png (4 subplots)
â”‚
â”œâ”€ Section 9: Quartile Analysis
â”‚  â””â”€ Print: Performance by quartile
â”‚
â”œâ”€ Section 10: Summary Report
â”‚  â””â”€ Save: PCA_summary_report.csv
â”‚
â”œâ”€ Section 11: Comprehensive Dashboard
â”‚  â””â”€ Plot: comprehensive_dashboard.png
â”‚     â”œâ”€ VUS-PR distribution
â”‚     â”œâ”€ VUS-ROC distribution
â”‚     â”œâ”€ Runtime distribution
â”‚     â”œâ”€ Category performance (top 15)
â”‚     â”œâ”€ VUS-PR vs Anomaly Ratio
â”‚     â”œâ”€ VUS-PR vs Data Length
â”‚     â””â”€ VUS-PR vs VUS-ROC
â”‚
â””â”€ Section 12: Conclusions
   â””â”€ Print: Key findings summary
```

## ğŸ” Comparison Tool Flow

```
Compare_PCA_with_Benchmark.py
â”‚
â”œâ”€â–º Load Data
â”‚   â”œâ”€ PCA results (PCA_all_results.csv)
â”‚   â””â”€ Benchmark results (uni_mergedTable_VUS-PR.csv)
â”‚
â”œâ”€â–º Align Datasets
â”‚   â”œâ”€ Find common datasets
â”‚   â””â”€ Merge on filename
â”‚
â”œâ”€â–º Calculate Rankings
â”‚   â”œâ”€ Mean VUS-PR for each method
â”‚   â””â”€ Sort descending
â”‚
â”œâ”€â–º Statistical Tests
â”‚   â”œâ”€ Wilcoxon signed-rank tests
â”‚   â”œâ”€ PCA vs top 5 methods
â”‚   â””â”€ Significance levels (*, **, ***)
â”‚
â”œâ”€â–º Generate Plots
â”‚   â”œâ”€ pca_vs_top_methods.png (box plot)
â”‚   â”œâ”€ method_ranking.png (bar chart)
â”‚   â””â”€ pca_vs_best.png (scatter)
â”‚
â””â”€â–º Save Results
    â””â”€ combined_results.csv
```

## ğŸ“ Metrics Explained Visually

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         VUS-PR (Primary Metric)             â”‚
â”‚                                             â”‚
â”‚  Z-axis: Precision                          â”‚
â”‚     â–²                                       â”‚
â”‚   1.0â”‚     â•±â•²  â† Surface                   â”‚
â”‚      â”‚   â•±    â•²                             â”‚
â”‚   0.5â”‚ â•±        â•²                           â”‚
â”‚      â”‚â•±__________â•²___                       â”‚
â”‚   0.0â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ X-axis: Recall     â”‚
â”‚       0    Window   100                     â”‚
â”‚            Size                             â”‚
â”‚                                             â”‚
â”‚  Volume = âˆ«âˆ« Precision Ã— dRecall Ã— dWindow â”‚
â”‚  Range: [0, 1], Higher = Better            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ† Success Indicators

```
After running the complete pipeline:

âœ… Files Check
   â€¢ 870 .npy files in scores/
   â€¢ PCA_all_results.csv exists (870 rows)
   â€¢ 8+ PNG plots generated
   â€¢ Summary report created

âœ… Data Sanity
   â€¢ Mean VUS-PR: 0.30-0.45
   â€¢ No NaN values in results
   â€¢ Runtime reasonable (<1s per dataset avg)

âœ… Analysis Complete
   â€¢ All notebook cells executed
   â€¢ No errors in output
   â€¢ Plots look reasonable

âœ… Comparison Done
   â€¢ PCA ranked among methods
   â€¢ Comparison plots generated
   â€¢ Statistical tests completed
```

## ğŸš¨ Common Issues & Solutions

```
Issue                        Solution
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
"File not found"          â†’ Check DATASET_DIR path
Memory error              â†’ Process in batches
Slow execution            â†’ Normal for 870 datasets
Unicode error             â†’ Check log file encoding
Missing benchmark         â†’ Comparison optional
CUDA warning              â†’ Ignore (PCA doesn't use GPU)
```

## ğŸ“Š Example Results Preview

```
Sample from PCA_all_results.csv:

file                               VUS-PR  VUS-ROC  time  category
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
001_NAB_id_1_Facility_*.csv       0.4521  0.6234  0.23   NAB
002_NAB_id_2_WebService_*.csv     0.3891  0.5876  0.18   NAB
003_NAB_id_3_WebService_*.csv     0.5234  0.7123  0.19   NAB
...

Summary Statistics:
  Total datasets: 870
  Mean VUS-PR: 0.3847 Â± 0.1523
  Best: 0.9123 (file: XXX)
  Worst: 0.0234 (file: YYY)
  Total runtime: 18.5 minutes
```

## ğŸ¯ Quick Reference Commands

```bash
# Run everything
python PCA_Quick_Start.py

# Just run PCA
python Run_PCA_Pipeline.py

# Just analyze
jupyter notebook PCA_Statistical_Analysis.ipynb

# Just compare
python Compare_PCA_with_Benchmark.py

# Check progress
tail -f eval/PCA_pipeline/PCA_pipeline.log

# View results
head eval/PCA_pipeline/metrics/PCA_all_results.csv
```

---

**Now you have a complete visual understanding of the pipeline! Ready to run it! ğŸš€**
